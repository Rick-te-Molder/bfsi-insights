{
  "url": "https://arxiv.org/pdf/2503.13657",
  "title": "Why Do Multi-Agent LLM Systems Fail?",
  "slug": "why-do-multi-agent-llm-systems-fail",
  "authors": [
    "Mert Cemri",
    "Melissa Z. Pan",
    "Shuyi Yang",
    "Lakshya A. Agrawal",
    "Bhavya Chopra",
    "Rishabh Tiwari",
    "Kurt Keutzer",
    "Aditya Parameswaran",
    "Dan Klein",
    "Kannan Ramchandran",
    "Matei Zaharia",
    "Joseph E. Gonzalez",
    "Ion Stoica"
  ],
  "source_name": "arXiv",
  "source_domain": "arxiv.org",
  "thumbnail": "/thumbs/2025_why-do-multi-agent-llm-systems-fail_cemri-arxiv.webp",
  "date_published": "2025-03-17",
  "date_added": "2025-10-28T19:00:00+01:00",
  "last_edited": "2025-10-28T19:00:00+01:00",
  "role": "academic",
  "industry": "cross-bfsi",
  "topic": "technology-and-data-agentic-engineering",
  "use_cases": "portfolio-analytics",
  "agentic_capabilities": "evaluation",
  "content_type": "peer-reviewed-paper",
  "jurisdiction": "global",
  "note": "This paper investigates why systems made up of multiple large-language-model (LLM) agents — so-called Multi-Agent Systems (MAS) — often underperform despite high expectations. The authors collected over 1,600 execution traces from seven MAS frameworks using modern LLMs (e.g., GPT-4, Claude, CodeLlama) and developed the first large dataset of MAS failures. They also defined a taxonomy of 14 failure modes grouped into three categories: (1) System design issues (e.g., unclear roles or steps repeated), (2) Inter-agent misalignment (e.g., agents ignoring each other or failing to ask clarifying questions), and (3) Task verification failures (e.g., incomplete or incorrect checks of output). They show that many failures stem not just from the LLM models themselves but from architectural, coordination and verification design. They release the dataset and taxonomy publicly to help guide better MAS design and research.",
  "id": "f0d1a8d15a063b7b9d3fd3cacd8c162f87623456"
}
